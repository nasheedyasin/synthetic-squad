{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I9tjPBOFEWY-"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-HiexQdEsss"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m6qO2nhvEp5v"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_tr.csv\")\n",
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC19L4x6G_BF",
        "outputId": "313781ed-826a-47ac-c147-084775c4cc4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'human': 0,\n",
              " 'fair': 1,\n",
              " 'grover': 2,\n",
              " 'gpt2': 3,\n",
              " 'gpt3': 4,\n",
              " 'instructgpt': 5,\n",
              " 'gpt': 6,\n",
              " 'ctrl': 7,\n",
              " 'pplm': 8,\n",
              " 'xlnet': 9,\n",
              " 'xlm': 10}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels2id = {alg:i for i, alg in enumerate(train_df[\"alg\"].unique())}\n",
        "labels2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NQWz86zHXXx",
        "outputId": "0e906fd8-00cf-4c5d-b915-3de352d91a0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'human',\n",
              " 1: 'fair',\n",
              " 2: 'grover',\n",
              " 3: 'gpt2',\n",
              " 4: 'gpt3',\n",
              " 5: 'instructgpt',\n",
              " 6: 'gpt',\n",
              " 7: 'ctrl',\n",
              " 8: 'pplm',\n",
              " 9: 'xlnet',\n",
              " 10: 'xlm'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2labels = {v:k for k, v in labels2id.items()}\n",
        "id2labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5wi-UKkdEteR"
      },
      "outputs": [],
      "source": [
        "def get_features_test(data, vectorizer=None, numerical_fields=None):\n",
        "  if not vectorizer:\n",
        "    raise Exception(\"Vectorizer need\")\n",
        "  \n",
        "  tfidf = vectorizer.get(\"tfidf\", {})\n",
        "  count_vec = vectorizer.get(\"count\", {})\n",
        "\n",
        "  features = []\n",
        "  if not numerical_fields:\n",
        "    numerical_fields = []\n",
        "\n",
        "  for ele in tfidf:\n",
        "    features.append(ele[1].transform(data[ele[0]]))\n",
        "\n",
        "  for ele in count_vec:\n",
        "    features.append(ele[1].transform(data[ele[0]]))\n",
        "\n",
        "  for field in numerical_fields:\n",
        "    features.append(scipy.sparse.csr_matrix(data[field]).T)\n",
        "\n",
        "  return scipy.sparse.hstack(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FzDnlHhZHO5y"
      },
      "outputs": [],
      "source": [
        "def test_model(model, X, y, id2labels):\n",
        "  preds = list(map(lambda x: id2labels[x], model.predict(X)))\n",
        "  preds = ['machine' if item != 'human' else item for item in preds]\n",
        "  true_label = y.replace(['fair', 'grover', 'gpt2', 'gpt3', 'instructgpt', 'gpt', 'ctrl', 'pplm', 'xlnet', 'xlm'],'machine')\n",
        "  return(classification_report(preds, true_label))\n",
        "  # return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5s4_vDxuF25M"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# trained only on generation text\n",
        "vectorizer_generation = joblib.load(\"../nlp-models/vectorizer_generation.pkl\")\n",
        "lsvc_generation = joblib.load(\"../nlp-models/lsvc_generation.pkl\")\n",
        "rf_generation = joblib.load(\"../nlp-models/rf_generation.pkl\")\n",
        "xgb_generation = joblib.load(\"../nlp-models/xgb_generation.pkl\")\n",
        "\n",
        "\n",
        "# trained only on pos tags of the text\n",
        "vectorizer_pos = joblib.load(\"../nlp-models/vectorizer_pos.pkl\")\n",
        "lsvc_pos = joblib.load(\"../nlp-models/lsvc_pos.pkl\")\n",
        "rf_pos = joblib.load(\"../nlp-models/rf_pos.pkl\")\n",
        "xgb_pos = joblib.load(\"../nlp-models/xgb_pos.pkl\")\n",
        "\n",
        "\n",
        "# trained on pos,dep tags and liwc like \"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\" of the text\n",
        "vectorizer_pos_dep_liwc = joblib.load(\"../nlp-models/vectorizer_pos_dep_liwc.pkl\")\n",
        "lsvc_pos_dep_liwc = joblib.load(\"../nlp-models/lsvc_pos_dep_liwc.pkl\")\n",
        "rf_pos_dep_liwc = joblib.load(\"../nlp-models/rf_pos_dep_liwc.pkl\")\n",
        "xgb_pos_dep_liwc = joblib.load(\"../nlp-models/xgb_pos_dep_liwc.pkl\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HreeABvaJHuT"
      },
      "source": [
        "## **On Generation Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y8xxI-8Ex2z",
        "outputId": "0cb6cf29-6639-438b-e8c4-4b865d43d063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------Linear SVC---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.81      0.77      0.79       222\n",
            "     machine       0.95      0.96      0.96      1057\n",
            "\n",
            "    accuracy                           0.93      1279\n",
            "   macro avg       0.88      0.87      0.87      1279\n",
            "weighted avg       0.93      0.93      0.93      1279\n",
            "\n",
            "----------------------Random Forest---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       1.00      0.92      0.96       232\n",
            "     machine       0.98      1.00      0.99      1047\n",
            "\n",
            "    accuracy                           0.99      1279\n",
            "   macro avg       0.99      0.96      0.97      1279\n",
            "weighted avg       0.99      0.99      0.98      1279\n",
            "\n",
            "----------------------XGBOOST---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.96      0.91      0.93       226\n",
            "     machine       0.98      0.99      0.99      1053\n",
            "\n",
            "    accuracy                           0.98      1279\n",
            "   macro avg       0.97      0.95      0.96      1279\n",
            "weighted avg       0.98      0.98      0.98      1279\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_features_generation = get_features_test(test_df, vectorizer=vectorizer_generation)\n",
        "\n",
        "# true_label = test_df[\"alg\"].replace(['fair', 'grover', 'gpt2', 'gpt3', 'instructgpt', 'gpt', 'ctrl', 'pplm', 'xlnet', 'xlm'],'machine')\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_generation, test_features_generation, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_generation, test_features_generation, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_generation, test_features_generation, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eCjJvVIeIgmg"
      },
      "outputs": [],
      "source": [
        "# same way for reddit dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y35kJA8WJSms"
      },
      "source": [
        "## **POS SEQUENCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DH4X1ZtIhGa",
        "outputId": "2429ad1c-4ec3-425f-efb2-b4dac5d3ce63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------Linear SVC---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.79      0.70      0.74       239\n",
            "     machine       0.93      0.96      0.94      1040\n",
            "\n",
            "    accuracy                           0.91      1279\n",
            "   macro avg       0.86      0.83      0.84      1279\n",
            "weighted avg       0.91      0.91      0.91      1279\n",
            "\n",
            "----------------------Random Forest---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.85      0.91      0.88       199\n",
            "     machine       0.98      0.97      0.98      1080\n",
            "\n",
            "    accuracy                           0.96      1279\n",
            "   macro avg       0.92      0.94      0.93      1279\n",
            "weighted avg       0.96      0.96      0.96      1279\n",
            "\n",
            "----------------------XGBOOST---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.88      0.91      0.90       207\n",
            "     machine       0.98      0.98      0.98      1072\n",
            "\n",
            "    accuracy                           0.97      1279\n",
            "   macro avg       0.93      0.94      0.94      1279\n",
            "weighted avg       0.97      0.97      0.97      1279\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "unvZYhD-IhJM"
      },
      "outputs": [],
      "source": [
        "# same for reddit data set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. CTRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHfw8jgSI7S8"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "filtered_test_df = test_df[test_df['alg'].isin(['ctrl', 'human'])]\n",
        "test_features_pos = get_features_test(filtered_test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, filtered_test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, filtered_test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, filtered_test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. FAIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['fair', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['gpt', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['gpt2', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['gpt3', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. GROVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['grover', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. InstructGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['instructgpt', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. PPLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['pplm', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. XLNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['xlnet', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. XLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['xlm', 'human'])]\n",
        "test_features_pos = get_features_test(test_df, vectorizer=vectorizer_pos)\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos, test_features_pos, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos, test_features_pos, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_3YH-D3BJbdc"
      },
      "source": [
        "# POS DEP AND LIWC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGHWIBawI7Vv",
        "outputId": "6423663b-423b-4b91-e08a-72ca00f5d32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------Linear SVC---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.82      1.00      0.90       175\n",
            "     machine       1.00      0.74      0.85       145\n",
            "\n",
            "    accuracy                           0.88       320\n",
            "   macro avg       0.91      0.87      0.88       320\n",
            "weighted avg       0.90      0.88      0.88       320\n",
            "\n",
            "----------------------Random Forest---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.85      1.00      0.92       182\n",
            "     machine       1.00      0.78      0.87       138\n",
            "\n",
            "    accuracy                           0.90       320\n",
            "   macro avg       0.93      0.89      0.90       320\n",
            "weighted avg       0.92      0.90      0.90       320\n",
            "\n",
            "----------------------XGBOOST---------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.91      1.00      0.95       194\n",
            "     machine       1.00      0.85      0.92       126\n",
            "\n",
            "    accuracy                           0.94       320\n",
            "   macro avg       0.96      0.92      0.94       320\n",
            "weighted avg       0.95      0.94      0.94       320\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pIsCi3nIWv4"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. CTRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['ctrl', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. FAIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['fair', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['gpt', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['gpt2', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['gpt3', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. GROVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['grover', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. InstructGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['instructgpt', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. PPLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['pplm', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. XLNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['xlnet', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. XLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_eval.csv\")\n",
        "reddit_df = pd.read_csv(\"../nlp-data/liwc_pos_dep_reddit.csv\")\n",
        "test_df = test_df[test_df['alg'].isin(['xlm', 'human'])]\n",
        "test_features_pos_dep_liwc = get_features_test(test_df, vectorizer=vectorizer_pos_dep_liwc, numerical_fields=[\"semantic_coherence\", \"Analytic\", \"WPS\", \"article\", \"Period\"])\n",
        "\n",
        "print(\"----------------------Linear SVC---------------------------\")\n",
        "print(test_model(lsvc_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------Random Forest---------------------------\")\n",
        "print(test_model(rf_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))\n",
        "\n",
        "print(\"----------------------XGBOOST---------------------------\")\n",
        "print(test_model(xgb_pos_dep_liwc, test_features_pos_dep_liwc, test_df[\"alg\"], id2labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjgskkRXPH7v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx1CdWv5PH-b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
